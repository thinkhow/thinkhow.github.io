{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Loading and process data-SIG-Sentiment analysis using RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thinkhow/thinkhow.github.io/blob/master/Loading_and_process_data_SIG_Sentiment_analysis_using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiFTM-xgDuE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cdbb997-faea-4f1a-ebeb-51ab38b9063a"
      },
      "source": [
        "#for saving files and trained models to save RAM\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_XFmDriZrh1"
      },
      "source": [
        "Install packages\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkToKPWOZquZ",
        "outputId": "d5160f61-18fe-4315-8296-1d9144b4f15a"
      },
      "source": [
        "pip install praw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/a8/a2e2d0750ee17c7e3d81e4695a0338ad0b3f231853b8c3fa339ff2d25c7c/praw-7.2.0-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 7.4MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/0c/d52a2a63512a613817846d430d16a8fbe5ea56dd889e89c68facf6b91cb6/websocket_client-0.59.0-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
            "\u001b[?25hCollecting prawcore<3,>=2\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/df/4a9106bea0d26689c4b309da20c926a01440ddaf60c09a5ae22684ebd35f/prawcore-2.0.0-py3-none-any.whl\n",
            "Collecting update-checker>=0.18\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2->praw) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (2020.12.5)\n",
            "Installing collected packages: websocket-client, prawcore, update-checker, praw\n",
            "Successfully installed praw-7.2.0 prawcore-2.0.0 update-checker-0.18.0 websocket-client-0.59.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8rvXR9rZUMu"
      },
      "source": [
        "Import packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlRHF14DWErb"
      },
      "source": [
        "import praw\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCYwud3jWNRR"
      },
      "source": [
        "Scrape Reddit data \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWgMQ8E0WSNG"
      },
      "source": [
        "# Acessing the reddit api\n",
        "\n",
        "reddit = praw.Reddit(client_id=\"8SMVmmj4NUo4hw\",      # your client id\n",
        "                     client_secret=\"jbmzvrX3heEFjtcDNBtIXeqUTlIT_g\",  #your client secret\n",
        "                     user_agent=\"Khoa Nguyen\", #user agent name\n",
        "                     username = \"hopernguyen\",     # your reddit username\n",
        "                     password = \"\")     # your reddit password"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucAFXPKsbiG_"
      },
      "source": [
        "sub = ['Askreddit']  # make a list of subreddits you want to scrape the data from\n",
        "\n",
        "for s in sub:\n",
        "    \n",
        "    subreddit = reddit.subreddit(s)   # Chosing the subreddit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "kDatMFYzbrKP",
        "outputId": "c7ebb3c5-47a5-42c9-8aaa-e3eed73dc6df"
      },
      "source": [
        "########################################\n",
        "#   CREATING DICTIONARY TO STORE THE DATA WHICH WILL BE CONVERTED TO A DATAFRAME\n",
        "########################################\n",
        "\n",
        "#   NOTE: ALL THE POST DATA AND COMMENT DATA WILL BE SAVED IN TWO DIFFERENT\n",
        "#   DATASETS AND LATER CAN BE MAPPED USING IDS OF POSTS/COMMENTS AS WE WILL \n",
        "#   BE CAPTURING ALL IDS THAT COME IN OUR WAY\n",
        "\n",
        "# SCRAPING CAN BE DONE VIA VARIOUS STRATEGIES {HOT,TOP,etc} we will go with keyword strategy i.e using search a keyword\n",
        "query = ['Gaming']\n",
        "for item in query:\n",
        "    post_dict = {\n",
        "        \"title\" : [],   #title of the post\n",
        "        \"score\" : [],   # score of the post\n",
        "        \"id\" : [],      # unique id of the post\n",
        "        \"url\" : [],     #url of the post\n",
        "        \"comms_num\": [],   #the number of comments on the post\n",
        "        \"created\" : [],  #timestamp of the post\n",
        "        \"body\" : []         # the descriptionof post\n",
        "    }\n",
        "    comments_dict = {\n",
        "        \"comment_id\" : [],      #unique comm id\n",
        "        \"comment_parent_id\" : [],   # comment parent id\n",
        "        \"comment_body\" : [],   # text in comment\n",
        "        \"comment_link_id\" : []  #link to the comment\n",
        "    }\n",
        "    for submission in subreddit.search(query,sort = \"top\",limit = 1):\n",
        "        post_dict[\"title\"].append(submission.title)\n",
        "        post_dict[\"score\"].append(submission.score)\n",
        "        post_dict[\"id\"].append(submission.id)\n",
        "        post_dict[\"url\"].append(submission.url)\n",
        "        post_dict[\"comms_num\"].append(submission.num_comments)\n",
        "        post_dict[\"created\"].append(submission.created)\n",
        "        post_dict[\"body\"].append(submission.selftext)\n",
        "        \n",
        "        ##### Acessing comments on the post\n",
        "        submission.comments.replace_more(limit = 1)\n",
        "        for comment in submission.comments.list():\n",
        "            comments_dict[\"comment_id\"].append(comment.id)\n",
        "            comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
        "            comments_dict[\"comment_body\"].append(comment.body)\n",
        "            comments_dict[\"comment_link_id\"].append(comment.link_id)\n",
        "    \n",
        "    post_comments = pd.DataFrame(comments_dict)\n",
        "\n",
        "    post_comments.to_csv(\"./drive/MyDrive/Erdo_Project/Data/\"+s+\"_comments_\"+ item +\"subreddit.csv\")\n",
        "    post_data = pd.DataFrame(post_dict)\n",
        "    post_data.to_csv(\"./drive/MyDrive/Erdos_Project/Data\"+s+\"_\"+ item +\"subreddit.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b27f92085dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mpost_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mpost_comments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./drive/MyDrive/Erdo_Project/Data/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_comments_\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"subreddit.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mpost_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mpost_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./drive/MyDrive/Erdos_Project/Data\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"subreddit.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[1;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m         )\n\u001b[0;32m-> 3170\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             )\n\u001b[1;32m    192\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/MyDrive/Erdo_Project/Data/Askreddit_comments_Gamingsubreddit.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnJZlvlOWymz"
      },
      "source": [
        "Scrape twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F992-YqJZbJO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-ysQj8UZchi"
      },
      "source": [
        "SPY data (Upload kaggle file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl9nO67WZh3b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbFRLtoCHi25"
      },
      "source": [
        "FinViz stock screener - Original article: https://towardsdatascience.com/stock-news-sentiment-analysis-with-python-193d4b4378d4 \n",
        "\n",
        "This shows how to import the market data by ticker using FinViz and some easy sentiment analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4CpXR3kJ1tk",
        "outputId": "8bf66452-be77-43ee-90e3-4177a70cc20c"
      },
      "source": [
        "pip install twython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twython\n",
            "  Downloading https://files.pythonhosted.org/packages/24/80/579b96dfaa9b536efde883d4f0df7ea2598a6f3117a6dd572787f4a2bcfb/twython-3.8.2-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2020.12.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.1.0)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1cfTnaoMmDx"
      },
      "source": [
        "The variable n represents the number of articles that will be displayed for each ticker in the ‘tickers’ list. The rest of the code will not have to be manually updated and these are the only parameters you will have to change each time you run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YLcbhJfH0OR",
        "outputId": "17fcb3bd-bd2c-4084-9bdf-428676abfd97"
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.request import urlopen\n",
        "from urllib.request import Request\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "# Parameters \n",
        "n = 3 #the # of article headlines displayed per ticker\n",
        "tickers = ['AAPL', 'TSLA', 'AMZN']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAqfhArlOJax"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2o-5Qn3Mwlp"
      },
      "source": [
        "we will get the news data from the FinViz website using the modules BeautifulSoup and requests. The code parses the URL for the HTML table of news and iterates through the list of tickers to gather the recent headlines for each ticker. For each inputted stock, an ‘n’ number of recent headlines is printed out so the data is easy to view.\n",
        "\n",
        "The news are typically from yahoo, barrons, motley fool, insider monkey,...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GCVCWzKJ0gY",
        "outputId": "8e3d0c6e-ef7d-45ab-ff2a-c5e3b16db498"
      },
      "source": [
        "# Get Data\n",
        "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
        "news_tables = {}\n",
        "\n",
        "for ticker in tickers:\n",
        "    url = finviz_url + ticker\n",
        "    req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
        "    resp = urlopen(req)    \n",
        "    html = BeautifulSoup(resp, features=\"lxml\")\n",
        "    news_table = html.find(id='news-table')\n",
        "    news_tables[ticker] = news_table\n",
        "\n",
        "try:\n",
        "    for ticker in tickers:\n",
        "        df = news_tables[ticker]\n",
        "        df_tr = df.findAll('tr')\n",
        "    \n",
        "        print ('\\n')\n",
        "        print ('Recent News Headlines for {}: '.format(ticker))\n",
        "        \n",
        "        for i, table_row in enumerate(df_tr):\n",
        "            a_text = table_row.a.text\n",
        "            td_text = table_row.td.text\n",
        "            td_text = td_text.strip()\n",
        "            print(a_text,'(',td_text,')')\n",
        "            if i == n-1:\n",
        "                break\n",
        "except KeyError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Recent News Headlines for AAPL: \n",
            "WFH opens up companies to more cyber threats: Microsoft ( May-12-21 11:04AM )\n",
            "Will Bill Gates Divorce Affect These Stocks? ( 10:42AM )\n",
            "4 Dow Jones Stocks To Buy And Watch In May 2021: Apple, Microsoft Sell Off ( 10:31AM )\n",
            "\n",
            "\n",
            "Recent News Headlines for TSLA: \n",
            "Dow Jones Slides, Tech Stocks Dive As Yields Jump On Accelerating Inflation; Apple, Tesla Sell Off ( May-12-21 11:02AM )\n",
            "ARK Invest Stocks To Buy And Watch: 6 Stocks That Cathie Wood's ARK ETFs Own; Sea Limited, Square, Tesla Slide ( 10:52AM )\n",
            "UPDATE 1-Tesla says it supports standardisation of China auto industry ( 09:17AM )\n",
            "\n",
            "\n",
            "Recent News Headlines for AMZN: \n",
            "WFH opens up companies to more cyber threats: Microsoft ( May-12-21 11:04AM )\n",
            "Will Bill Gates Divorce Affect These Stocks? ( 10:42AM )\n",
            "Amazon wins court fight over $300M EU tax order ( 10:00AM )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udQNz7YEJoWt"
      },
      "source": [
        "# Iterate through the news\n",
        "parsed_news = []\n",
        "for file_name, news_table in news_tables.items():\n",
        "    for x in news_table.findAll('tr'):\n",
        "        text = x.a.get_text() \n",
        "        date_scrape = x.td.text.split()\n",
        "\n",
        "        if len(date_scrape) == 1:\n",
        "            time = date_scrape[0]\n",
        "            \n",
        "        else:\n",
        "            date = date_scrape[0]\n",
        "            time = date_scrape[1]\n",
        "\n",
        "        ticker = file_name.split('_')[0]\n",
        "        \n",
        "        parsed_news.append([ticker, date, time, text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXx0EvHyLGa0"
      },
      "source": [
        "Each headline is analyzed for its polarity score on a scale of -1 to 1, with -1 being highly negative and highly 1 being positive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHsNrlfJKpdc"
      },
      "source": [
        "\n",
        "# Sentiment Analysis\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "columns = ['Ticker', 'Date', 'Time', 'Headline']\n",
        "news = pd.DataFrame(parsed_news, columns=columns)\n",
        "scores = news['Headline'].apply(analyzer.polarity_scores).tolist()\n",
        "\n",
        "df_scores = pd.DataFrame(scores)\n",
        "news = news.join(df_scores, rsuffix='_right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgVoZPR0LVP2"
      },
      "source": [
        " For each ticker in the inputted list, a new DataFrame will be created that includes its headlines and their respective scores\n",
        "\n",
        " DataFrame will be created that includes each ticker’s mean sentiment value over all the recent news parsed.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "47RavVdeLWAA",
        "outputId": "fc1292bf-f7cd-4278-cc9d-c127d9b54bb0"
      },
      "source": [
        "# View Data \n",
        "news['Date'] = pd.to_datetime(news.Date).dt.date\n",
        "\n",
        "unique_ticker = news['Ticker'].unique().tolist()\n",
        "news_dict = {name: news.loc[news['Ticker'] == name] for name in unique_ticker}\n",
        "\n",
        "values = []\n",
        "for ticker in tickers: \n",
        "    dataframe = news_dict[ticker]\n",
        "    dataframe = dataframe.set_index('Ticker')\n",
        "    dataframe = dataframe.drop(columns = ['Headline'])\n",
        "    print ('\\n')\n",
        "    print (dataframe.head())\n",
        "    \n",
        "    mean = round(dataframe['compound'].mean(), 2)\n",
        "    values.append(mean)\n",
        "    \n",
        "df = pd.DataFrame(list(zip(tickers, values)), columns =['Ticker', 'Mean Sentiment']) \n",
        "df = df.set_index('Ticker')\n",
        "df = df.sort_values('Mean Sentiment', ascending=False)\n",
        "print ('\\n')\n",
        "print (df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-84e664a6cfc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# View Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munique_ticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ticker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnews_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ticker'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_ticker\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcAAdEH4LrLZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIgaPtCYOLQa"
      },
      "source": [
        "Recurrent Neural Networks and Long Short Term Memory (RNN and LSTM). The idea is we use the sentiment analysis to produce the score (unsupervised), use RNN to fit the score to the market movement (supervised). For testing, we first calculate the sentiment for the test info (unsupervised), feed it to the trained RRN, and predict the market. \n",
        "Reference: https://arxiv.org/pdf/1705.02447.pdf\n",
        "Github: https://github.com/irfanICMLL/EMM-for-stock-prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCqjOlGEDqFl"
      },
      "source": [
        "Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcOOT88UDsDS"
      },
      "source": [
        "from torch import nn\n",
        "from torch import optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JLXhDIFYAB4"
      },
      "source": [
        "#once training and test data is defined, using this cell to load into the dataloader for automatic batching\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEoQgpf1Fbfm"
      },
      "source": [
        "n=100 #number of posts/news examined by sentiment analysis for a stock at particular time\n",
        "input_size = n+1 #change based on size of input vector for X (i.e. sentiment analysis for n posts, plus 1 for volatility )\n",
        "\n",
        "#neural network inside sequential container (in case we want to add more layers)\n",
        "model = nn.Sequential(\n",
        "    nn.LSTM(input_size=input_size, hidden_size=25), #hidden size is the output dimension of the LSTM cell\n",
        "    nn.Linear(25,1) #(input, output) want a seperate output layer so that we can have more lstm cells (i.e. neurons) in the hidden layer\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzW8EykWYjHB"
      },
      "source": [
        "learning_rate =0.001 #default for adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate) #using default first and second moment values (i.e. betas in documentation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx91RD6eay_X"
      },
      "source": [
        "loss_fn = nn.MSELoss() #Mean squared error loss since output is continuous "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7ef7OTpcO4C"
      },
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    pred =model(X)  #we might need to add something for the recurrent hidden output idk, this is just one of the examples\n",
        "    loss =loss_fn(pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GffvjQuKbKZD"
      },
      "source": [
        "#https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for X, y in dataloader:\n",
        "        pred = model(X)\n",
        "        test_loss += loss_fn(pred, y).item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= size\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og-blbXcXvLC"
      },
      "source": [
        "#https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "epochs=10\n",
        "for i in range(epochs):\n",
        "  print(f\"Epoch {i}\\n---------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn) #validation set function call to check generalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT9cYaQwZhO4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}