{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Loading and process data-SIG-Sentiment analysis using RNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thinkhow/thinkhow.github.io/blob/master/Loading_and_process_data_SIG_Sentiment_analysis_using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_XFmDriZrh1"
      },
      "source": [
        "Install packages\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkToKPWOZquZ",
        "outputId": "558de531-ee6f-4cd7-ca38-bdeb06853c52"
      },
      "source": [
        "pip install praw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/a8/a2e2d0750ee17c7e3d81e4695a0338ad0b3f231853b8c3fa339ff2d25c7c/praw-7.2.0-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 8.6MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/0c/d52a2a63512a613817846d430d16a8fbe5ea56dd889e89c68facf6b91cb6/websocket_client-0.59.0-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \n",
            "\u001b[?25hCollecting prawcore<3,>=2\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/df/4a9106bea0d26689c4b309da20c926a01440ddaf60c09a5ae22684ebd35f/prawcore-2.0.0-py3-none-any.whl\n",
            "Collecting update-checker>=0.18\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2->praw) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (3.0.4)\n",
            "Installing collected packages: websocket-client, prawcore, update-checker, praw\n",
            "Successfully installed praw-7.2.0 prawcore-2.0.0 update-checker-0.18.0 websocket-client-0.59.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8rvXR9rZUMu"
      },
      "source": [
        "Import packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlRHF14DWErb"
      },
      "source": [
        "import praw\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCYwud3jWNRR"
      },
      "source": [
        "Scrape Reddit data \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWgMQ8E0WSNG"
      },
      "source": [
        "# Acessing the reddit api\n",
        "\n",
        "reddit = praw.Reddit(client_id=\"8SMVmmj4NUo4hw\",      # your client id\n",
        "                     client_secret=\"jbmzvrX3heEFjtcDNBtIXeqUTlIT_g\",  #your client secret\n",
        "                     user_agent=\"Khoa Nguyen\", #user agent name\n",
        "                     username = \"hopernguyen\",     # your reddit username\n",
        "                     password = \"\")     # your reddit password"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucAFXPKsbiG_"
      },
      "source": [
        "sub = ['Askreddit']  # make a list of subreddits you want to scrape the data from\n",
        "\n",
        "for s in sub:\n",
        "    \n",
        "    subreddit = reddit.subreddit(s)   # Chosing the subreddit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDatMFYzbrKP",
        "outputId": "3d75054a-6e1b-4122-e56d-b42905253e94"
      },
      "source": [
        "########################################\n",
        "#   CREATING DICTIONARY TO STORE THE DATA WHICH WILL BE CONVERTED TO A DATAFRAME\n",
        "########################################\n",
        "\n",
        "#   NOTE: ALL THE POST DATA AND COMMENT DATA WILL BE SAVED IN TWO DIFFERENT\n",
        "#   DATASETS AND LATER CAN BE MAPPED USING IDS OF POSTS/COMMENTS AS WE WILL \n",
        "#   BE CAPTURING ALL IDS THAT COME IN OUR WAY\n",
        "\n",
        "# SCRAPING CAN BE DONE VIA VARIOUS STRATEGIES {HOT,TOP,etc} we will go with keyword strategy i.e using search a keyword\n",
        "query = ['Gaming']\n",
        "for item in query:\n",
        "    post_dict = {\n",
        "        \"title\" : [],   #title of the post\n",
        "        \"score\" : [],   # score of the post\n",
        "        \"id\" : [],      # unique id of the post\n",
        "        \"url\" : [],     #url of the post\n",
        "        \"comms_num\": [],   #the number of comments on the post\n",
        "        \"created\" : [],  #timestamp of the post\n",
        "        \"body\" : []         # the descriptionof post\n",
        "    }\n",
        "    comments_dict = {\n",
        "        \"comment_id\" : [],      #unique comm id\n",
        "        \"comment_parent_id\" : [],   # comment parent id\n",
        "        \"comment_body\" : [],   # text in comment\n",
        "        \"comment_link_id\" : []  #link to the comment\n",
        "    }\n",
        "    for submission in subreddit.search(query,sort = \"top\",limit = 1):\n",
        "        post_dict[\"title\"].append(submission.title)\n",
        "        post_dict[\"score\"].append(submission.score)\n",
        "        post_dict[\"id\"].append(submission.id)\n",
        "        post_dict[\"url\"].append(submission.url)\n",
        "        post_dict[\"comms_num\"].append(submission.num_comments)\n",
        "        post_dict[\"created\"].append(submission.created)\n",
        "        post_dict[\"body\"].append(submission.selftext)\n",
        "        \n",
        "        ##### Acessing comments on the post\n",
        "        submission.comments.replace_more(limit = 1)\n",
        "        for comment in submission.comments.list():\n",
        "            comments_dict[\"comment_id\"].append(comment.id)\n",
        "            comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
        "            comments_dict[\"comment_body\"].append(comment.body)\n",
        "            comments_dict[\"comment_link_id\"].append(comment.link_id)\n",
        "    \n",
        "    post_comments = pd.DataFrame(comments_dict)\n",
        "\n",
        "    post_comments.to_csv(s+\"_comments_\"+ item +\"subreddit.csv\")\n",
        "    post_data = pd.DataFrame(post_dict)\n",
        "    post_data.to_csv(s+\"_\"+ item +\"subreddit.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnJZlvlOWymz"
      },
      "source": [
        "Scrape twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F992-YqJZbJO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-ysQj8UZchi"
      },
      "source": [
        "SPY data (Upload kaggle file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl9nO67WZh3b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbFRLtoCHi25"
      },
      "source": [
        "FinViz stock screener - Original article: https://towardsdatascience.com/stock-news-sentiment-analysis-with-python-193d4b4378d4 \n",
        "\n",
        "This shows how to import the market data by ticker using FinViz and some easy sentiment analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4CpXR3kJ1tk",
        "outputId": "86b1a6c0-6ee8-4227-8e04-db979482729a"
      },
      "source": [
        "pip install twython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twython\n",
            "  Downloading https://files.pythonhosted.org/packages/24/80/579b96dfaa9b536efde883d4f0df7ea2598a6f3117a6dd572787f4a2bcfb/twython-3.8.2-py3-none-any.whl\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1cfTnaoMmDx"
      },
      "source": [
        "The variable n represents the number of articles that will be displayed for each ticker in the ‘tickers’ list. The rest of the code will not have to be manually updated and these are the only parameters you will have to change each time you run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YLcbhJfH0OR",
        "outputId": "d4b6de18-0d6a-4b5c-946b-55bdb9a0d1bc"
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.request import urlopen\n",
        "from urllib.request import Request\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "# Parameters \n",
        "n = 3 #the # of article headlines displayed per ticker\n",
        "tickers = ['AAPL', 'TSLA', 'AMZN']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2o-5Qn3Mwlp"
      },
      "source": [
        "we will get the news data from the FinViz website using the modules BeautifulSoup and requests. The code parses the URL for the HTML table of news and iterates through the list of tickers to gather the recent headlines for each ticker. For each inputted stock, an ‘n’ number of recent headlines is printed out so the data is easy to view.\n",
        "\n",
        "The news are typically from yahoo, barrons, motley fool, insider monkey,...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GCVCWzKJ0gY",
        "outputId": "8e3d0c6e-ef7d-45ab-ff2a-c5e3b16db498"
      },
      "source": [
        "# Get Data\n",
        "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
        "news_tables = {}\n",
        "\n",
        "for ticker in tickers:\n",
        "    url = finviz_url + ticker\n",
        "    req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
        "    resp = urlopen(req)    \n",
        "    html = BeautifulSoup(resp, features=\"lxml\")\n",
        "    news_table = html.find(id='news-table')\n",
        "    news_tables[ticker] = news_table\n",
        "\n",
        "try:\n",
        "    for ticker in tickers:\n",
        "        df = news_tables[ticker]\n",
        "        df_tr = df.findAll('tr')\n",
        "    \n",
        "        print ('\\n')\n",
        "        print ('Recent News Headlines for {}: '.format(ticker))\n",
        "        \n",
        "        for i, table_row in enumerate(df_tr):\n",
        "            a_text = table_row.a.text\n",
        "            td_text = table_row.td.text\n",
        "            td_text = td_text.strip()\n",
        "            print(a_text,'(',td_text,')')\n",
        "            if i == n-1:\n",
        "                break\n",
        "except KeyError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Recent News Headlines for AAPL: \n",
            "WFH opens up companies to more cyber threats: Microsoft ( May-12-21 11:04AM )\n",
            "Will Bill Gates Divorce Affect These Stocks? ( 10:42AM )\n",
            "4 Dow Jones Stocks To Buy And Watch In May 2021: Apple, Microsoft Sell Off ( 10:31AM )\n",
            "\n",
            "\n",
            "Recent News Headlines for TSLA: \n",
            "Dow Jones Slides, Tech Stocks Dive As Yields Jump On Accelerating Inflation; Apple, Tesla Sell Off ( May-12-21 11:02AM )\n",
            "ARK Invest Stocks To Buy And Watch: 6 Stocks That Cathie Wood's ARK ETFs Own; Sea Limited, Square, Tesla Slide ( 10:52AM )\n",
            "UPDATE 1-Tesla says it supports standardisation of China auto industry ( 09:17AM )\n",
            "\n",
            "\n",
            "Recent News Headlines for AMZN: \n",
            "WFH opens up companies to more cyber threats: Microsoft ( May-12-21 11:04AM )\n",
            "Will Bill Gates Divorce Affect These Stocks? ( 10:42AM )\n",
            "Amazon wins court fight over $300M EU tax order ( 10:00AM )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udQNz7YEJoWt"
      },
      "source": [
        "# Iterate through the news\n",
        "parsed_news = []\n",
        "for file_name, news_table in news_tables.items():\n",
        "    for x in news_table.findAll('tr'):\n",
        "        text = x.a.get_text() \n",
        "        date_scrape = x.td.text.split()\n",
        "\n",
        "        if len(date_scrape) == 1:\n",
        "            time = date_scrape[0]\n",
        "            \n",
        "        else:\n",
        "            date = date_scrape[0]\n",
        "            time = date_scrape[1]\n",
        "\n",
        "        ticker = file_name.split('_')[0]\n",
        "        \n",
        "        parsed_news.append([ticker, date, time, text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXx0EvHyLGa0"
      },
      "source": [
        "Each headline is analyzed for its polarity score on a scale of -1 to 1, with -1 being highly negative and highly 1 being positive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHsNrlfJKpdc"
      },
      "source": [
        "\n",
        "# Sentiment Analysis\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "columns = ['Ticker', 'Date', 'Time', 'Headline']\n",
        "news = pd.DataFrame(parsed_news, columns=columns)\n",
        "scores = news['Headline'].apply(analyzer.polarity_scores).tolist()\n",
        "\n",
        "df_scores = pd.DataFrame(scores)\n",
        "news = news.join(df_scores, rsuffix='_right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgVoZPR0LVP2"
      },
      "source": [
        " For each ticker in the inputted list, a new DataFrame will be created that includes its headlines and their respective scores\n",
        "\n",
        " DataFrame will be created that includes each ticker’s mean sentiment value over all the recent news parsed.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47RavVdeLWAA",
        "outputId": "b643fc40-e3ba-4f70-ecff-bd6b0ff5a41d"
      },
      "source": [
        "# View Data \n",
        "news['Date'] = pd.to_datetime(news.Date).dt.date\n",
        "\n",
        "unique_ticker = news['Ticker'].unique().tolist()\n",
        "news_dict = {name: news.loc[news['Ticker'] == name] for name in unique_ticker}\n",
        "\n",
        "values = []\n",
        "for ticker in tickers: \n",
        "    dataframe = news_dict[ticker]\n",
        "    dataframe = dataframe.set_index('Ticker')\n",
        "    dataframe = dataframe.drop(columns = ['Headline'])\n",
        "    print ('\\n')\n",
        "    print (dataframe.head())\n",
        "    \n",
        "    mean = round(dataframe['compound'].mean(), 2)\n",
        "    values.append(mean)\n",
        "    \n",
        "df = pd.DataFrame(list(zip(tickers, values)), columns =['Ticker', 'Mean Sentiment']) \n",
        "df = df.set_index('Ticker')\n",
        "df = df.sort_values('Mean Sentiment', ascending=False)\n",
        "print ('\\n')\n",
        "print (df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "              Date     Time    neg    neu    pos  compound\n",
            "Ticker                                                    \n",
            "AAPL    2021-05-12  11:04AM  0.278  0.722  0.000   -0.4728\n",
            "AAPL    2021-05-12  10:42AM  0.000  1.000  0.000    0.0000\n",
            "AAPL    2021-05-12  10:31AM  0.000  1.000  0.000    0.0000\n",
            "AAPL    2021-05-12  09:47AM  0.000  0.777  0.223    0.3182\n",
            "AAPL    2021-05-12  08:30AM  0.000  1.000  0.000    0.0000\n",
            "\n",
            "\n",
            "              Date     Time    neg    neu    pos  compound\n",
            "Ticker                                                    \n",
            "TSLA    2021-05-12  11:02AM  0.000  1.000  0.000    0.0000\n",
            "TSLA    2021-05-12  10:52AM  0.095  0.905  0.000   -0.2263\n",
            "TSLA    2021-05-12  09:17AM  0.000  0.783  0.217    0.3612\n",
            "TSLA    2021-05-12  09:00AM  0.000  0.762  0.238    0.3612\n",
            "TSLA    2021-05-12  08:54AM  0.000  1.000  0.000    0.0000\n",
            "\n",
            "\n",
            "              Date     Time    neg    neu    pos  compound\n",
            "Ticker                                                    \n",
            "AMZN    2021-05-12  11:04AM  0.278  0.722  0.000   -0.4728\n",
            "AMZN    2021-05-12  10:42AM  0.000  1.000  0.000    0.0000\n",
            "AMZN    2021-05-12  10:00AM  0.186  0.429  0.386    0.4215\n",
            "AMZN    2021-05-12  09:07AM  0.000  0.892  0.108    0.1779\n",
            "AMZN    2021-05-12  09:00AM  0.000  0.805  0.195    0.1779\n",
            "\n",
            "\n",
            "        Mean Sentiment\n",
            "Ticker                \n",
            "AMZN              0.10\n",
            "AAPL              0.04\n",
            "TSLA              0.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcAAdEH4LrLZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}